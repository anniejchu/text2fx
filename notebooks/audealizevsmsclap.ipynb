{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d445e0c7-a27e-4e83-9515-4ce2374df4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from typing import Union, List\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import audiotools\n",
    "import dasp_pytorch\n",
    "import auraloss\n",
    "# import laion_clap\n",
    "from audiotools import AudioSignal\n",
    "\n",
    "from transformers import BertForMaskedLM\n",
    "\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import helper\n",
    "\n",
    "# dir(helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92413cdc-e88d-4535-85d5-e27e99aeeb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/annie/research/text2fx/assets\n"
     ]
    }
   ],
   "source": [
    "NOTEBOOKS_DIR = Path(os.path.abspath(''))\n",
    "PROJECT_DIR = NOTEBOOKS_DIR.parent\n",
    "ASSETS_DIR = PROJECT_DIR / \"assets\"\n",
    "PRETRAINED_DIR = PROJECT_DIR / \"pretrained\"\n",
    "DATA_DIR = PROJECT_DIR / \"data\"\n",
    "RUNS_DIR = PROJECT_DIR / \"runs\"\n",
    "EXPERIMENTS_DIR = PROJECT_DIR / \"experiments\"\n",
    "\n",
    "\n",
    "EXP_AUDEALIZE_DIR = EXPERIMENTS_DIR / \"audealize_comp\"\n",
    "EXPORT_EXAMPLES_DIR = Path(EXP_AUDEALIZE_DIR / \"audealize_version\")\n",
    "\n",
    "print(ASSETS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567881bb-2cfc-4e80-b6f2-781b93164c71",
   "metadata": {},
   "source": [
    "### Setting Up Audealize Ground Truth Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d019ba42-2711-4eb7-b6d3-aff3c88bbaec",
   "metadata": {},
   "source": [
    "##### Loading in Audealize API / word <> EQ settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5d2e333-cc22-4163-941a-0999acac4494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON data Audealize API word <> EQ gain values\n",
    "file_path = '/home/annie/research/text2fx/notebooks/audealize_data/eqdescriptors.json'\n",
    "top10_eq = [\"warm\", \"cold\", \"soft\", \"loud\", \"happy\", \"bright\", \"soothing\", \"harsh\", \"heavy\", \"cool\"]\n",
    "settings_dict = helper.get_settings_for_words(file_path, top10_eq)\n",
    "# print(settings_dict['cool'])\n",
    "\n",
    "# Zipping (frequency bands, gain vals) in dictionary\n",
    "freq_bands = [20, 50, 83, 120, 161, 208, 259, 318, 383, 455, 537, 628, 729, 843, 971, \n",
    "              1114, 1273, 1452, 1652, 1875, 2126, 2406, 2719, 3070, 3462, 3901, \n",
    "              4392, 4941, 5556, 6244, 7014, 7875, 8839, 9917, 11124, 12474, 13984, \n",
    "              15675, 17566, 19682]\n",
    "\n",
    "converted_settings_dict = helper.convert_to_freq_gain_tuples(settings_dict, freq_bands)\n",
    "# print(converted_settings_dict['cold'])\n",
    "\n",
    "# Converting all parameters into tensors\n",
    "tensor_settings = helper.convert_to_tensors(converted_settings_dict)\n",
    "# print(tensor_settings['cold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "308af542-5baa-44e9-8800-9e3cdf2cba68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('/home/annie/research/text2fx/assets/audealize_examples/drums.wav'), PosixPath('/home/annie/research/text2fx/assets/audealize_examples/guitar.wav'), PosixPath('/home/annie/research/text2fx/assets/audealize_examples/piano.wav')]\n"
     ]
    }
   ],
   "source": [
    "# Loading audealize ground truth files\n",
    "all_audealize_samples = helper.load_and_find_path_with_keyword(ASSETS_DIR, [\"audealize\"], returnSingle=False)\n",
    "# all_audealize_samples = helper.load_and_find_path_with_keyword(ASSETS_DIR, [\"audealize\", \"piano\"], returnSingle=False)\n",
    "\n",
    "print(all_audealize_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84cd5a6-02fc-422d-b9be-cd38f010c06a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Generating Audealize Gnd Truth Examples, uncomment to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a0cd68e-be35-4343-a459-3289aeda0144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving warm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/annie/miniconda3/envs/css_wespeaker/lib/python3.9/site-packages/audiotools/core/audio_signal.py:601: UserWarning: Audio amplitude > 1 clipped when saving\n",
      "  warnings.warn(\"Audio amplitude > 1 clipped when saving\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving cold\n",
      "saving soft\n",
      "saving loud\n",
      "saving happy\n",
      "saving bright\n",
      "saving soothing\n",
      "saving harsh\n",
      "saving heavy\n",
      "saving cool\n"
     ]
    }
   ],
   "source": [
    "# # Original Input\n",
    "# audio_type = \"speech\"\n",
    "\n",
    "# #loading clean file\n",
    "# input_raw= helper.load_and_find_path_with_keyword(ASSETS_DIR, [\"225\"], returnSingle=True) #searches for file\n",
    "# input_sig = AudioSignal(input_raw).to_mono()\n",
    "\n",
    "# # generating output files\n",
    "# for word, freq_gains in tensor_settings.items():\n",
    "#     filtered_sig, fs = helper.dasp_apply_EQ_file(input_raw, freq_gains)\n",
    "#     filter_out = AudioSignal(filtered_sig,input_sig.sample_rate)\n",
    "#     print(f'saving {word}')\n",
    "\n",
    "#     EXPORT_EX_DIR = Path(EXPORT_EXAMPLES_DIR / f\"{audio_type}\")\n",
    "#     EXPORT_EX_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "#     filter_out.write(Path(EXPORT_EX_DIR, f\"{word}.wav\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3246a846-4e88-4907-ad0b-f45e2ae23862",
   "metadata": {},
   "source": [
    "### Comparing Audealize Files with Other Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bca6e7-48de-4537-ab02-423a1896191d",
   "metadata": {},
   "source": [
    "### Audealize vs MS CLAP: \n",
    "- FX: Just EQ\n",
    "- Words: Top 10 Frequent EQ words (warm 64, cold 34, soft 29, loud 26, happy 22, bright 19, soothing 17, harsh 16, heavy 15, cool 14)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347549e8-b44e-40d7-883e-fa12be92561c",
   "metadata": {},
   "source": [
    "##### Audio Type: Guitar Riff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69db915d-79c8-4692-a816-eb4cdeb74bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "keyword = 'speech'\n",
    "# Loading Ground Truth Paths\n",
    "audealize_out_ALL = helper.load_and_find_path_with_keyword(EXPORT_EXAMPLES_DIR, [f'{keyword}'], returnSingle=False)\n",
    "print(len(audealize_out_ALL)) #checking length, should be number of words aka 10\n",
    "# for path in audealize_out_ALL:\n",
    "#     print(path)\n",
    "\n",
    "# Loading all MS CLAP output files\n",
    "MS_CLAP_OUTPUTS = EXP_AUDEALIZE_DIR / \"ms_clap\" \n",
    "msclap_out_ALL = helper.load_and_find_path_with_keyword(MS_CLAP_OUTPUTS, [f'{keyword}', \"final\"])\n",
    "print(len(msclap_out_ALL))  #checking length, should be number of words aka 10\n",
    "# for path in msclap_out_ALL:\n",
    "#     print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b0f90ab-2867-4754-829a-04635746b511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/annie/research/text2fx/experiments/audealize_comp/audealize_version/guitar/warm.wav\n",
      "/home/annie/research/text2fx/experiments/audealize_comp/ms_clap/guitar/this_sound_is_warm/final.wav\n"
     ]
    }
   ],
   "source": [
    "target_word = 'warm'\n",
    "\n",
    "audealize_out_word = helper.load_and_find_path_with_keyword(EXPORT_EXAMPLES_DIR, [\"guitar\", f\"{target_word}\"], returnSingle=True)\n",
    "msclap_out_word = helper.load_and_find_path_with_keyword(MS_CLAP_OUTPUTS, [\"guitar\", \"final\", f\"{target_word}\"], returnSingle=True)\n",
    "\n",
    "print(audealize_out_word)\n",
    "print(msclap_out_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41491b1f-c916-4961-924a-58f198b608c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.3112)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helper.compare_loss_anyfiles(audealize_out_word, msclap_out_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18b1fe39-ea1c-4436-ab6a-fd33499740b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_losses(audio_type, output_dir=MS_CLAP_OUTPUTS, verbose=True):\n",
    "    losses = []\n",
    "    for word in top10_eq:\n",
    "        target_word = word\n",
    "        audealize_out_word = helper.load_and_find_path_with_keyword(EXPORT_EXAMPLES_DIR, [f\"{audio_type}\", f\"{target_word}\"], returnSingle=True)\n",
    "        msclap_out_word = helper.load_and_find_path_with_keyword(output_dir, [f\"{audio_type}\", \"final\", f\"{target_word}\"], returnSingle=True)\n",
    "        if verbose:\n",
    "            print(f'AUDEALIZE_OUT PATH: {audealize_out_word}')\n",
    "            print(f'MS_CLAP_OUT PATH: {msclap_out_word}')\n",
    "\n",
    "        loss = helper.compare_loss_anyfiles(audealize_out_word, msclap_out_word)\n",
    "        losses.append((word, loss))\n",
    "        print(audio_type, word, loss)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46dbf68a-dd5d-4a7d-90e2-bfe806bd3f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guitar warm tensor(7.3112)\n",
      "guitar cold tensor(2.0035)\n",
      "guitar soft tensor(2.0129)\n",
      "guitar loud tensor(4.6183)\n",
      "guitar happy tensor(2.5699)\n",
      "guitar bright tensor(9.2267)\n",
      "guitar soothing tensor(1.3768)\n",
      "guitar harsh tensor(3.9483)\n",
      "guitar heavy tensor(3.5747)\n",
      "guitar cool tensor(1.4394)\n"
     ]
    }
   ],
   "source": [
    "losses = calculate_losses(\"guitar\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fe2c72c-3eff-46c6-acca-ba30cf5f6ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drums warm tensor(1.6901)\n",
      "drums cold tensor(1.0105)\n",
      "drums soft tensor(3.5292)\n",
      "drums loud tensor(2.5581)\n",
      "drums happy tensor(1.3005)\n",
      "drums bright tensor(2.5930)\n",
      "drums soothing tensor(1.8868)\n",
      "drums harsh tensor(2.3616)\n",
      "drums heavy tensor(4.1008)\n",
      "drums cool tensor(2.0344)\n"
     ]
    }
   ],
   "source": [
    "losses = calculate_losses(\"drums\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5b35d2a-b0fc-4566-bf51-6abcc0def5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piano warm tensor(1.4968)\n",
      "piano cold tensor(1.9354)\n",
      "piano soft tensor(2.5631)\n",
      "piano loud tensor(4.7190)\n",
      "piano happy tensor(2.0785)\n",
      "piano bright tensor(6.0401)\n",
      "piano soothing tensor(2.0321)\n",
      "piano harsh tensor(4.4739)\n",
      "piano heavy tensor(3.2222)\n",
      "piano cool tensor(1.9128)\n"
     ]
    }
   ],
   "source": [
    "losses = calculate_losses(\"piano\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83059fd9-66fe-4d0d-864c-1a1a7706fc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for speech, need an extra arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1241b70-2621-42c2-9308-dac145947eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_losses_superlatives(audio_type, output_dir=MS_CLAP_OUTPUTS, verbose=True):\n",
    "    losses = []\n",
    "    for word in top10_eq:\n",
    "        target_word = word\n",
    "        audealize_out_word = helper.load_and_find_path_with_keyword(EXPORT_EXAMPLES_DIR, [f\"{audio_type}\", f\"{target_word}\"], returnSingle=True)\n",
    "        msclap_out_word = helper.load_and_find_path_with_keyword(output_dir, [f\"{audio_type}\", \"final\", f\"{target_word}\", \"normal\"], returnSingle=True)\n",
    "        if verbose:\n",
    "            print(f'AUDEALIZE_OUT PATH: {audealize_out_word}')\n",
    "            print(f'MS_CLAP_OUT PATH: {msclap_out_word}')\n",
    "\n",
    "        loss = helper.compare_loss_anyfiles(audealize_out_word, msclap_out_word)\n",
    "        losses.append((word, loss))\n",
    "        print(audio_type, word, loss)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a57664e9-04f4-4d62-be91-78fb9d8e12f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speech warm tensor(1.6165)\n",
      "speech cold tensor(1.5096)\n",
      "speech soft tensor(2.4517)\n",
      "speech loud tensor(3.8767)\n",
      "speech happy tensor(1.7991)\n",
      "speech bright tensor(1.7889)\n",
      "speech soothing tensor(1.4891)\n",
      "speech harsh tensor(2.5302)\n",
      "speech heavy tensor(3.7243)\n",
      "speech cool tensor(1.7167)\n"
     ]
    }
   ],
   "source": [
    "losses = calculate_losses_superlatives(\"speech\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b0d017-cabc-44bd-bf4e-32a5fe5d17c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text2fx",
   "language": "python",
   "name": "text2fx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
